{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/atlas/data19/guhitj/Erdos_DL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('Erdos_v2/Erdos-2024-DL-Newsworthy/finetune_roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpu_index = 1  # Change to 1 if you want to use the second GPU\n",
    "#device = torch.device(f\"cuda:{gpu_index}\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(seed=42):\n",
    "    dataset_openai = load_dataset('csv', data_files='news_openai_final.csv')\n",
    "\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_val_test_split = dataset_openai['train'].train_test_split(test_size=0.2, seed=seed)\n",
    "    train_val_split = train_val_test_split['train'].train_test_split(test_size=0.25, seed=seed)\n",
    "\n",
    "    dataset = DatasetDict({\n",
    "        'train': train_val_split['train'].shuffle(seed=seed),  # 60% of the original data\n",
    "        'validation': train_val_split['test'].shuffle(seed=seed),  # 20% of the original data\n",
    "        'test': train_val_test_split['test'].shuffle(seed=seed),  # 20% of the original data\n",
    "    })\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Publishing Time', 'Ticker', 'Sector', 'Source', 'Headline', 'Text', 'openai_sentiment', 'openai_score'],\n",
       "    num_rows: 38221\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = prepare_data()\n",
    "dataset['validation']\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sentiment_model import SentimentModel\n",
    "from SentimentDataModule import SentimentDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate without fine-tuning roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module_roberta = SentimentDataModule(dataset['train'], dataset['validation'], 8,  512)\n",
    "data_module_roberta.setup()\n",
    "val_loader_roberta = data_module_roberta.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    "#    for batch in val_loader_roberta:\n",
    "#        print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: -1, 1: 0, 2: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions_roberta = 0\n",
    "total_predictions_roberta = 0\n",
    "all_predictions_roberta = []\n",
    "all_labels_roberta = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in val_loader_roberta:\n",
    "        #print(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Compute predictions\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        predictions_mapped = torch.tensor([label_map[pred.item()] for pred in predictions]).to(device)\n",
    "        labels_mapped = torch.tensor([label_map[label.item()] for label in labels]).to(device)\n",
    "\n",
    "        correct_predictions_roberta += (predictions_mapped == labels_mapped).sum().item()\n",
    "        total_predictions_roberta += labels_mapped.size(0)\n",
    "        all_predictions_roberta.extend(predictions_mapped.cpu().numpy())\n",
    "        all_labels_roberta.extend(labels_mapped.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Roberta: 0.5452\n",
      "Classification Report Roberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Class -1       0.00      0.00      0.00      2488\n",
      "     Class 0       0.00      0.00      0.00      3306\n",
      "     Class 1       0.55      1.00      0.71      6947\n",
      "\n",
      "    accuracy                           0.55     12741\n",
      "   macro avg       0.18      0.33      0.24     12741\n",
      "weighted avg       0.30      0.55      0.38     12741\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/atlas/data19/guhitj/micromamba/envs/erdos_2024_dl_newsworthy/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/atlas/data19/guhitj/micromamba/envs/erdos_2024_dl_newsworthy/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/atlas/data19/guhitj/micromamba/envs/erdos_2024_dl_newsworthy/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "accuracy_roberta = correct_predictions_roberta / total_predictions_roberta\n",
    "report_roberta = classification_report(all_labels_roberta, all_predictions_roberta, target_names=['Class -1', 'Class 0', 'Class 1'])\n",
    "\n",
    "print(f'Accuracy Roberta: {accuracy_roberta:.4f}')\n",
    "print(f'Classification Report Roberta:\\n{report_roberta}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating with finetuned roberta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/atlas/data19/guhitj/micromamba/envs/erdos_2024_dl_newsworthy/lib/python3.11/site-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#checkpoint_path = '/lustre/umt3/user/guhitj/Erdos_bootcamp/Deeplearning/Project/Results/NewRun/checkpoints/Run_15_20240822-181659_FullRun30EFineTune/epoch=07-val_loss=0.34250.ckpt'\n",
    "#checkpoint_path = '/lustre/umt3/user/guhitj/Erdos_bootcamp/Deeplearning/Project/Results/NewRun/checkpoints/Run_14_20240822-042913_FullRun30E/epoch=18-val_loss=0.41.ckpt'\n",
    "#checkpoint_path = '/lustre/umt3/user/guhitj/Erdos_bootcamp/Deeplearning/Project/Results/NewRun/checkpoints/Run_14_20240822-042913_FullRun30E/epoch=06-val_loss=0.72.ckpt'\n",
    "#checkpoint_path = '/lustre/umt3/user/guhitj/Erdos_bootcamp/Deeplearning/Project/Results/NewRun/checkpoints/Run_15_20240822-181659_FullRun30EFineTune/epoch=12-val_loss=0.34323.ckpt'\n",
    "checkpoint_path = '/lustre/umt3/user/guhitj/Erdos_bootcamp/Deeplearning/Project/Results/NewRun/checkpoints/Run_16_20240825-085343_FullRun20EFineTune/epoch=06-val_loss=0.32016.ckpt'\n",
    "model_finetuned = SentimentModel.load_from_checkpoint(checkpoint_path)\n",
    "model_finetuned.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_finetuned = model_finetuned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module_finetuned = SentimentDataModule(dataset['train'], dataset['validation'], 8,  512)\n",
    "data_module_finetuned.setup()\n",
    "val_loader_finetuned = data_module_finetuned.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions_finetuned = 0\n",
    "total_predictions_finetuned = 0\n",
    "all_predictions_finetuned = []\n",
    "all_labels_finetuned = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in val_loader_finetuned:\n",
    "        #print(batch)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model_finetuned(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[1]\n",
    "        #print(logits)\n",
    "      \n",
    "        # Compute predictions\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        predictions_mapped = torch.tensor([label_map[pred.item()] for pred in predictions]).to(device)\n",
    "        labels_mapped = torch.tensor([label_map[label.item()] for label in labels]).to(device)\n",
    "\n",
    "        correct_predictions_finetuned += (predictions_mapped == labels_mapped).sum().item()\n",
    "        total_predictions_finetuned += labels_mapped.size(0)\n",
    "        all_predictions_finetuned.extend(predictions_mapped.cpu().numpy())\n",
    "        all_labels_finetuned.extend(labels_mapped.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Roberta: 0.8838\n",
      "Classification Report Roberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Class -1       0.85      0.91      0.88      2488\n",
      "     Class 0       0.76      0.82      0.79      3306\n",
      "     Class 1       0.96      0.90      0.93      6947\n",
      "\n",
      "    accuracy                           0.88     12741\n",
      "   macro avg       0.86      0.88      0.87     12741\n",
      "weighted avg       0.89      0.88      0.89     12741\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_finetuned = correct_predictions_finetuned / total_predictions_finetuned\n",
    "report_finetuned = classification_report(all_labels_finetuned, all_predictions_finetuned, target_names=['Class -1', 'Class 0', 'Class 1'])\n",
    "\n",
    "print(f'Accuracy Roberta: {accuracy_finetuned:.4f}')\n",
    "print(f'Classification Report Roberta:\\n{report_finetuned}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from datasets import Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_file = 'news_openai_final.csv'\n",
    "df = pd.read_csv(news_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SentimentDataModule_all import SentimentDataModule_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module_fullDS = SentimentDataModule_all(dataset, 8,  512)\n",
    "data_module_fullDS.setup()\n",
    "data_loader_fullDS = data_module_fullDS.dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions_fullDS = 0\n",
    "total_predictions_fullDS = 0\n",
    "all_predictions_fullDS = []\n",
    "all_labels_fullDS = []\n",
    "\n",
    "neg_sentiment = []\n",
    "neutral_sentiment = []\n",
    "positive_sentiment = []\n",
    "compound_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions_fullDS = 0\n",
    "total_predictions_fullDS = 0\n",
    "all_predictions_fullDS = []\n",
    "all_labels_fullDS = []\n",
    "\n",
    "neg_sentiment = []\n",
    "neutral_sentiment = []\n",
    "positive_sentiment = []\n",
    "compound_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader_fullDS:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Get the logits from the model\n",
    "        outputs = model_finetuned(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs[1]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Process each sequence in the batch individually\n",
    "        for i in range(probs.size(0)):  # Loop over each sequence in the batch\n",
    "            prob_tensor = probs[i]  # Get the probabilities for the i-th sequence\n",
    "\n",
    "            # Extract the individual probabilities\n",
    "            negative_prob = prob_tensor[0].item()\n",
    "            neutral_prob = prob_tensor[1].item()\n",
    "            positive_prob = prob_tensor[2].item()\n",
    "\n",
    "            # Calculate the compound score\n",
    "            compound_score = positive_prob - negative_prob\n",
    "\n",
    "            # Append probabilities and compound score to respective lists\n",
    "            neg_sentiment.append(negative_prob)\n",
    "            neutral_sentiment.append(neutral_prob)\n",
    "            positive_sentiment.append(positive_prob)\n",
    "            compound_scores.append(compound_score)\n",
    "\n",
    "        # Compute predictions for each sequence in the batch\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        predictions_mapped = torch.tensor([label_map[pred.item()] for pred in predictions]).to(device)\n",
    "        labels_mapped = torch.tensor([label_map[label.item()] for label in labels]).to(device)\n",
    "\n",
    "        correct_predictions_fullDS += (predictions_mapped == labels_mapped).sum().item()\n",
    "        total_predictions_fullDS += labels_mapped.size(0)\n",
    "        all_predictions_fullDS.extend(predictions_mapped.cpu().numpy())\n",
    "        all_labels_fullDS.extend(labels_mapped.cpu().numpy())\n",
    "\n",
    "        # Debugging: Print lengths after each batch to ensure consistency\n",
    "        #print(f\"Batch processed. Lengths: neg_sentiment={len(neg_sentiment)}, all_predictions_fullDS={len(all_predictions_fullDS)}\")\n",
    "\n",
    "# Final assertion to ensure consistency\n",
    "assert len(neg_sentiment) == len(all_predictions_fullDS), \"Mismatch in lengths!\"\n",
    "assert len(neutral_sentiment) == len(all_predictions_fullDS), \"Mismatch in lengths!\"\n",
    "assert len(positive_sentiment) == len(all_predictions_fullDS), \"Mismatch in lengths!\"\n",
    "assert len(compound_scores) == len(all_predictions_fullDS), \"Mismatch in lengths!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Roberta: 0.8829\n",
      "Classification Report Roberta:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Class -1       0.87      0.91      0.89     12912\n",
      "     Class 0       0.76      0.83      0.79     16501\n",
      "     Class 1       0.96      0.90      0.93     34290\n",
      "\n",
      "    accuracy                           0.88     63703\n",
      "   macro avg       0.86      0.88      0.87     63703\n",
      "weighted avg       0.89      0.88      0.88     63703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_fullDS = correct_predictions_fullDS / total_predictions_fullDS\n",
    "report_fullDS = classification_report(all_labels_fullDS, all_predictions_fullDS, target_names=['Class -1', 'Class 0', 'Class 1'])\n",
    "\n",
    "print(f'Accuracy Roberta: {accuracy_fullDS:.4f}')\n",
    "print(f'Classification Report Roberta:\\n{report_fullDS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63703"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_predictions_fullDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63703, 63703, 63703, 63703)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_sentiment), len(neutral_sentiment), len(positive_sentiment), len(compound_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_predictions_fullDS) == len(df)\n",
    "assert len(neg_sentiment) == len(df)\n",
    "assert len(neutral_sentiment) == len(df)\n",
    "assert len(positive_sentiment) == len(df)\n",
    "assert len(compound_scores) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['frob_sentiment'] = all_predictions_fullDS\n",
    "df['frob_neg'] = neg_sentiment\n",
    "df['frob_neu'] = neutral_sentiment\n",
    "df['frob_pos'] = positive_sentiment\n",
    "df['frob_comp'] = compound_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['frob_sentiment'] = df['frob_sentiment'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publishing Time</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>openai_sentiment</th>\n",
       "      <th>openai_score</th>\n",
       "      <th>frob_sentiment</th>\n",
       "      <th>frob_neg</th>\n",
       "      <th>frob_neu</th>\n",
       "      <th>frob_pos</th>\n",
       "      <th>frob_comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-15 10:46:42+00:00</td>\n",
       "      <td>WFC</td>\n",
       "      <td>Finance</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Did Wells Fargo CEO Tim Sloan Earn His $1 Mill...</td>\n",
       "      <td>We learned this week that the scandal-plagued ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.165805</td>\n",
       "      <td>0.757393</td>\n",
       "      <td>0.076802</td>\n",
       "      <td>-0.089003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-15 10:47:26+00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Technology</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Don't Underestimate Apple's iPhone Business</td>\n",
       "      <td>The segment is an invaluable asset to Apple's ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033501</td>\n",
       "      <td>0.138107</td>\n",
       "      <td>0.828392</td>\n",
       "      <td>0.794891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-15 11:33:00+00:00</td>\n",
       "      <td>MA</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>A Closer Look At Mastercard's Key Value Drivers</td>\n",
       "      <td>Mastercard has consistently beat street estima...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.008662</td>\n",
       "      <td>0.990883</td>\n",
       "      <td>0.990427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-15 11:52:45+00:00</td>\n",
       "      <td>BAC</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Benzinga</td>\n",
       "      <td>Jim Cramer Gives His Opinion On Bank Of Americ...</td>\n",
       "      <td>On CNBC's \"Mad Money Lightning Round\", Jim Cra...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.715720</td>\n",
       "      <td>0.242757</td>\n",
       "      <td>0.041524</td>\n",
       "      <td>-0.674196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-15 13:29:39+00:00</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Benzinga</td>\n",
       "      <td>Uber And Waymo Seeking Outside Funding For Aut...</td>\n",
       "      <td>Commercially viable autonomous vehicle (AV) te...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032898</td>\n",
       "      <td>0.915692</td>\n",
       "      <td>0.051410</td>\n",
       "      <td>0.018511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Publishing Time Ticker      Sector           Source  \\\n",
       "0  2019-03-15 10:46:42+00:00    WFC     Finance  The Motley Fool   \n",
       "1  2019-03-15 10:47:26+00:00   AAPL  Technology  The Motley Fool   \n",
       "2  2019-03-15 11:33:00+00:00     MA     Finance           Forbes   \n",
       "3  2019-03-15 11:52:45+00:00    BAC     Finance         Benzinga   \n",
       "4  2019-03-15 13:29:39+00:00  GOOGL  Technology         Benzinga   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Did Wells Fargo CEO Tim Sloan Earn His $1 Mill...   \n",
       "1        Don't Underestimate Apple's iPhone Business   \n",
       "2    A Closer Look At Mastercard's Key Value Drivers   \n",
       "3  Jim Cramer Gives His Opinion On Bank Of Americ...   \n",
       "4  Uber And Waymo Seeking Outside Funding For Aut...   \n",
       "\n",
       "                                                Text  openai_sentiment  \\\n",
       "0  We learned this week that the scandal-plagued ...              -1.0   \n",
       "1  The segment is an invaluable asset to Apple's ...               1.0   \n",
       "2  Mastercard has consistently beat street estima...               1.0   \n",
       "3  On CNBC's \"Mad Money Lightning Round\", Jim Cra...               1.0   \n",
       "4  Commercially viable autonomous vehicle (AV) te...               0.0   \n",
       "\n",
       "   openai_score  frob_sentiment  frob_neg  frob_neu  frob_pos  frob_comp  \n",
       "0         -0.50             0.0  0.165805  0.757393  0.076802  -0.089003  \n",
       "1          0.75             1.0  0.033501  0.138107  0.828392   0.794891  \n",
       "2          0.80             1.0  0.000455  0.008662  0.990883   0.990427  \n",
       "3          0.50            -1.0  0.715720  0.242757  0.041524  -0.674196  \n",
       "4          0.10             0.0  0.032898  0.915692  0.051410   0.018511  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Publishing Time', 'Ticker', 'Sector', 'Source', 'Headline', 'Text',\n",
       "       'openai_sentiment', 'openai_score', 'frob_sentiment', 'frob_neg',\n",
       "       'frob_neu', 'frob_pos', 'frob_comp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reorder = [\n",
    "    'Publishing Time',\n",
    "    'Ticker',\n",
    "    'Sector',\n",
    "    'Source', \n",
    "    'Headline',\n",
    "    'Text',\n",
    "    'frob_sentiment',\n",
    "    'frob_comp',\n",
    "    'frob_neg',\n",
    "    'frob_neu',\n",
    "    'frob_pos',\n",
    "    'openai_sentiment',\n",
    "    'openai_score'\n",
    "]\n",
    "\n",
    "df = df[reorder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Publishing Time</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>frob_sentiment</th>\n",
       "      <th>frob_comp</th>\n",
       "      <th>frob_neg</th>\n",
       "      <th>frob_neu</th>\n",
       "      <th>frob_pos</th>\n",
       "      <th>openai_sentiment</th>\n",
       "      <th>openai_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-15 10:46:42+00:00</td>\n",
       "      <td>WFC</td>\n",
       "      <td>Finance</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Did Wells Fargo CEO Tim Sloan Earn His $1 Mill...</td>\n",
       "      <td>We learned this week that the scandal-plagued ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089003</td>\n",
       "      <td>0.165805</td>\n",
       "      <td>0.757393</td>\n",
       "      <td>0.076802</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-15 10:47:26+00:00</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Technology</td>\n",
       "      <td>The Motley Fool</td>\n",
       "      <td>Don't Underestimate Apple's iPhone Business</td>\n",
       "      <td>The segment is an invaluable asset to Apple's ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.794891</td>\n",
       "      <td>0.033501</td>\n",
       "      <td>0.138107</td>\n",
       "      <td>0.828392</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-15 11:33:00+00:00</td>\n",
       "      <td>MA</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Forbes</td>\n",
       "      <td>A Closer Look At Mastercard's Key Value Drivers</td>\n",
       "      <td>Mastercard has consistently beat street estima...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990427</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.008662</td>\n",
       "      <td>0.990883</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-15 11:52:45+00:00</td>\n",
       "      <td>BAC</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Benzinga</td>\n",
       "      <td>Jim Cramer Gives His Opinion On Bank Of Americ...</td>\n",
       "      <td>On CNBC's \"Mad Money Lightning Round\", Jim Cra...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.674196</td>\n",
       "      <td>0.715720</td>\n",
       "      <td>0.242757</td>\n",
       "      <td>0.041524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-15 13:29:39+00:00</td>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Benzinga</td>\n",
       "      <td>Uber And Waymo Seeking Outside Funding For Aut...</td>\n",
       "      <td>Commercially viable autonomous vehicle (AV) te...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018511</td>\n",
       "      <td>0.032898</td>\n",
       "      <td>0.915692</td>\n",
       "      <td>0.051410</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Publishing Time Ticker      Sector           Source  \\\n",
       "0  2019-03-15 10:46:42+00:00    WFC     Finance  The Motley Fool   \n",
       "1  2019-03-15 10:47:26+00:00   AAPL  Technology  The Motley Fool   \n",
       "2  2019-03-15 11:33:00+00:00     MA     Finance           Forbes   \n",
       "3  2019-03-15 11:52:45+00:00    BAC     Finance         Benzinga   \n",
       "4  2019-03-15 13:29:39+00:00  GOOGL  Technology         Benzinga   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  Did Wells Fargo CEO Tim Sloan Earn His $1 Mill...   \n",
       "1        Don't Underestimate Apple's iPhone Business   \n",
       "2    A Closer Look At Mastercard's Key Value Drivers   \n",
       "3  Jim Cramer Gives His Opinion On Bank Of Americ...   \n",
       "4  Uber And Waymo Seeking Outside Funding For Aut...   \n",
       "\n",
       "                                                Text  frob_sentiment  \\\n",
       "0  We learned this week that the scandal-plagued ...             0.0   \n",
       "1  The segment is an invaluable asset to Apple's ...             1.0   \n",
       "2  Mastercard has consistently beat street estima...             1.0   \n",
       "3  On CNBC's \"Mad Money Lightning Round\", Jim Cra...            -1.0   \n",
       "4  Commercially viable autonomous vehicle (AV) te...             0.0   \n",
       "\n",
       "   frob_comp  frob_neg  frob_neu  frob_pos  openai_sentiment  openai_score  \n",
       "0  -0.089003  0.165805  0.757393  0.076802              -1.0         -0.50  \n",
       "1   0.794891  0.033501  0.138107  0.828392               1.0          0.75  \n",
       "2   0.990427  0.000455  0.008662  0.990883               1.0          0.80  \n",
       "3  -0.674196  0.715720  0.242757  0.041524               1.0          0.50  \n",
       "4   0.018511  0.032898  0.915692  0.051410               0.0          0.10  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_path = 'news_frob_wprobs_e06_val_0_32016.csv'  # Replace with your desired output path\n",
    "df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
